#
# Extensions
#

# This is not the full list of Druid extensions, but common ones that people often use. You may need to change this list
# based on your particular setup.
druid.extensions.loadList=[{{range $index, $service := .Values.global.druid.extensions.loadList}}{{if gt $index 0}}, {{end}}"{{.}}"{{end}}]

# If you have a different version of Hadoop, place your Hadoop client jar files in your hadoop-dependencies directory
# and uncomment the line below to point to your directory.
#druid.extensions.hadoopDependenciesDir=/my/dir/hadoop-dependencies

#
# Logging
#

# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.startup.logging.logProperties={{.Values.global.druid.startup.logging.logProperties}}

#
# Zookeeper
#

druid.zk.service.host={{.Values.global.druid.zk.service.host}}
druid.zk.paths.base={{.Values.global.druid.zk.paths.base}}

#
# Metadata storage
#

druid.metadata.storage.type={{.Values.global.druid.metadata.storage.type}}
druid.metadata.storage.connector.connectURI={{.Values.global.druid.metadata.storage.connector.connectURI}}
{{- if eq .Values.global.druid.metadata.storage.type "derby" }}
druid.metadata.storage.connector.host={{.Values.global.druid.metadata.storage.connector.host}}
druid.metadata.storage.connector.port={{.Values.global.druid.metadata.storage.connector.port}}
{{- else }}
druid.metadata.storage.connector.user={{.Values.global.druid.metadata.storage.connector.user}}
druid.metadata.storage.connector.password={{.Values.global.druid.metadata.storage.connector.password}}
{{- end }}

#
# Deep storage
#

# For local disk (only viable in a cluster if this is a network mount):
druid.storage.type={{.Values.global.druid.storage.type}}
{{- if eq .Values.global.druid.storage.type "s3" }}
druid.storage.bucket={{.Values.global.druid.storage.bucket}}
druid.storage.baseKey={{.Values.global.druid.storage.baseKey}}
druid.storage.useS3aSchema={{.Values.global.druid.storage.useS3aSchema}}
{{- else }}
druid.storage.storageDirectory={{.Values.global.druid.storage.storageDirectory}}
{{- end }}

{{- if .Values.global.druid.s3 }}
druid.s3.accessKey={{.Values.global.druid.s3.accessKey}}
druid.s3.secretKey={{.Values.global.druid.s3.secretKey}}
{{- end }}

#
# Indexing service logs
#

# For local disk (only viable in a cluster if this is a network mount):
druid.indexer.logs.type={{.Values.global.druid.indexer.logs.type}}
{{- if eq .Values.global.druid.indexer.logs.type "s3" }}
druid.indexer.logs.s3Bucket={{.Values.global.druid.indexer.logs.s3Bucket}}
druid.indexer.logs.s3Prefix={{.Values.global.druid.indexer.logs.s3Prefix}}
{{- else }}
druid.indexer.logs.directory={{.Values.global.druid.indexer.logs.directory}}
{{- end }}

#
# Service discovery
#

druid.selectors.indexing.serviceName={{.Values.global.druid.selectors.indexing.serviceName}}
druid.selectors.coordinator.serviceName={{.Values.global.druid.selectors.coordinator.serviceName}}

#
# Monitoring
#

druid.monitoring.monitors=[{{range $index, $service := .Values.global.druid.monitoring.monitors}}{{if gt $index 0}}, {{end}}"{{.}}"{{end}}]
# we use composing emitters to allow flexibility
druid.emitter=composing
druid.emitter.composing.emitters=[{{range $index, $service := .Values.global.druid.emitter.composing.emitters}}{{if gt $index 0}}, {{end}}"{{.}}"{{end}}]
druid.emitter.logging.logLevel={{.Values.global.druid.emitter.logging.logLevel}}

{{- if .Values.global.monitoring.enabled }}
druid.emitter.statsd.hostname=localhost
druid.emitter.statsd.port=9125
druid.emitter.statsd.includeHost=false
druid.emitter.statsd.separator=.
druid.emitter.statsd.dimensionMapPath=/dimensions.json
druid.emitter.statsd.prefix={{ .Values.global.monitoring.name }}
{{- end }}

# Storage type of double columns
# ommiting this will lead to index double as float at the storage layer

druid.indexing.doubleStorage={{.Values.global.druid.indexing.doubleStorage}}

# Javascript for postprocessing
druid.javascript.enabled={{.Values.global.druid.javascript.enabled}}

# built-in SQL layer
druid.sql.enable = {{.Values.global.druid.sql.enable}}
